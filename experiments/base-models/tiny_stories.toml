[training]
epochs = 4
learning_rate = 5e-4
learning_rate_scheduler = "cosine"
weight_decay = 0.1
gradient_accumulation_steps = 16
warmup_steps = 100
adam_betas = [0.9, 0.95]
sample_prompt = "Once upon a time"

[training.wandb]
project = "tiny-stories"
notes = "Tiny stories pre-training."
tags = ["tiny-stories", "1L"]

[training.dataset_config]
dataset_id = "roneneldan/TinyStories"
dataset_text_key = "text"
tokenizer_id = "nilq/baby-tokenizer"
batch_size = 64
block_size = 128
test_percentage = 0.15

[training.transformer_config]
vocab_dim = 20001
embedding_dim = 256
max_seq_len = 512
num_layers = 1
num_heads = 16
