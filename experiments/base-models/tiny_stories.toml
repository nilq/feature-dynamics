[training]
epochs = 4
learning_rate = 5e-4
learning_rate_scheduler = "cosine"
weight_decay = 0.1
gradient_accumulation_steps = 1
warmup_steps = 10
adam_betas = [0.9, 0.999]
sample_prompt = "My name is Niels"
log_interval = 100
save_interval = 5000

[training.wandb]
project = "tiny-stories"
notes = "Tiny stories pre-training."
tags = ["tiny-stories", "1L"]

[training.dataset_config]
dataset_id = "roneneldan/TinyStories"
dataset_text_key = "text"
tokenizer_id = "nilq/baby-tokenizer"
batch_size = 64
block_size = 512
test_percentage = 0.15

[training.transformer_config]
vocab_dim = 20001
embedding_dim = 1024
max_seq_len = 512
num_layers = 1
num_heads = 16
