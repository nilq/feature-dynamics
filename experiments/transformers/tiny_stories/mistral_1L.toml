[training]
epochs = 4
learning_rate = 5e-4
learning_rate_scheduler = "cosine"
weight_decay = 0.1
gradient_accumulation_steps = 1
warmup_steps = 0
adam_beta1 = 0.9
adam_beta2 = 0.999
output_dir = "./"
logging_steps = 1

[training.model_config]
model_type = "mistral"

[training.model_config.model_config_overrides]
hidden_size = 512
intermediate_size = 1024
num_attention_heads = 16
num_hidden_layers = 1
max_position_embeddings = 2048
attention_dropout = 0

[training.wandb]
project = "tiny-stories"
notes = "Mistral-like 1-layer model."
tags = ["mistral", "1L"]

[training.dataset_config]
dataset_id = "roneneldan/TinyStories"
dataset_text_key = "text"
tokenizer_id = "mistralai/Mistral-7B-v0.1"
block_size = 256
use_syntaxi = false
