[training]
learning_rate = 4e-4
adam_beta1 = 0.9
adam_beta2 = 0.99
dictionary_multiplier = 64
reconstruction_loss_sample_amount = 1000
use_ghost_gradients = true
dead_feature_window = 5000
seed = 1337

[training.model]
l1_coefficient = 1e-4
torch_dtype = "float32"
tied = false

[training.wandb]
project = "sparse-autoencoder"
notes = "Dry-run."
tags = ["Sparse Autoencoder", "mistral-1L-tiny"]

[training.data]
dataset_id = "roneneldan/TinyStories"
dataset_text_key = "text"
dataset_split = "validation"
target_layer = 0
target_model_name = "nilq/mistral-1L-tiny"
target_activation_name = "blocks.0.mlp.hook_post"
batch_size = 64
shuffle = true
tokenizer_id = "mistralai/Mistral-7B-v0.1"
block_size = 256
