[training]
learning_rate = 4e-4
adam_beta1 = 0.9
adam_beta2 = 0.99
dictionary_multiplier = 64
reconstruction_loss_sample_amount = 1000
use_ghost_gradients = true
dead_feature_window = 5000
seed = 1337

[training.model]
l1_coefficient = 8e-5
torch_dtype = "float32"
tied = false

[training.wandb]
project = "sparse-autoencoder"
notes = "Dry-run."
tags = ["Sparse Autoencoder", "gpt2"]

[training.data]
dataset_id = "roneneldan/TinyStories"
dataset_text_key = "text"
dataset_split = "validation"
target_layer = 2
target_model_name = "gpt2"
target_activation_name = "blocks.2.hook_resid_pre"
batch_size = 32
shuffle = true
tokenizer_id = "mistralai/Mistral-7B-v0.1"
block_size = 256
