[training]
learning_rate = 1e-4
adam_beta1 = 0.9
adam_beta2 = 0.9999
dictionary_multiplier = 16
reconstruction_loss_sample_amount = 5000
use_ghost_gradients = true
dead_feature_window = 1000
seed = 1337

[training.model]
l1_coefficient = 3e-4
torch_dtype = "float32"
tied = false

[training.wandb]
project = "sae-baby-python-mistral-1L-tiny-lua-ft"
notes = "Like mistral-1L-tiny/tinystories_small_batch"
tags = ["baby-python-lua", "16x dictionary", "trained on training data activations"]

[training.data]
dataset_id = "nilq/baby-python-and-lua"
dataset_text_key = "content"
dataset_split = "train"
validation_percentage = 0.2
target_layer = 0
target_model_name = "nilq/baby-python-mistral-1L-tiny-lua-ft"
target_activation_name = "blocks.0.mlp.hook_post"
batch_size = 128
shuffle = true
tokenizer_id = "mistralai/Mistral-7B-v0.1"
block_size = 24
