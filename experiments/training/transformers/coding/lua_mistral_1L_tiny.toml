[training]
num_train_epochs = 4
learning_rate = 6e-4
lr_scheduler_type = "cosine"
weight_decay = 0.1
gradient_accumulation_steps = 1
warmup_steps = 0
adam_beta1 = 0.9
adam_beta2 = 0.999
output_dir = "./lua-mistral-2L-tiny"
logging_steps = 1
save_steps=5000
save_total_limit=1000
per_device_train_batch_size = 64
push_to_hub = true

[training.model_config]
model_type = "mistral"
torch_dtype = "bfloat16"

[training.model_config.model_config_overrides]
hidden_size = 512
intermediate_size = 1024
num_attention_heads = 16
num_hidden_layers = 1
max_position_embeddings = 2048
attention_dropout = 0

[training.wandb]
project = "coding"
notes = "Mistral-like 1-layer model."
tags = ["mistral", "1L", "lua"]

[training.dataset_config]
dataset_id = "nilq/small-lua-stack"
dataset_text_key = "content"
tokenizer_id = "mistralai/Mistral-7B-v0.1"
block_size = 256
use_syntaxi = false

