[training]
epochs = 10
learning_rate = 2e-3
learning_rate_scheduler = "cosine"
weight_decay = 0.1
gradient_accumulation_steps = 1
warmup_steps = 100

[training.wandb]
project = "test"
notes = "Testing simple test run."
tags = ["amazing", "cool test"]

[training.dataset_config]
dataset_id = "nilq/babylm-100M"
dataset_text_key = "text"
tokenizer_id = "nilq/baby-tokenizer-uncased"
batch_size = 32
validation_percentage = 0.2
test_percentage = 0.1
use_syntaxi = false
block_size = 128

[training.transformer_config]
vocab_dim = 20001
embedding_dim = 256
max_seq_len = 128
num_layers = 1
num_heads = 8

