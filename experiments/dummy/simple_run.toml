[training]
epochs = 1
learning_rate = 6e-4
learning_rate_scheduler = "cosine"
weight_decay = 0.1
gradient_accumulation_steps = 16
warmup_steps = 10

[training.wandb]
project = "test"
notes = "Testing simple test run."
tags = ["amazing", "cool test"]

[training.dataset_config]
dataset_id = "nilq/babylm-10M"
dataset_text_key = "text"
tokenizer_id = "nilq/baby-tokenizer-uncased"
batch_size = 16
validation_percentage = 0.2
test_percentage = 0.1
use_syntaxi = true

[training.transformer_config]
vocab_dim = 20001
embedding_dim = 256
max_seq_len = 1024
num_layers = 1
num_heads = 8
